{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Why to bother with GPU computing in 2024**\n",
    "  * HPC and Supercomputing is GPU-accelerated\n",
    "  * When Julia overcomes the two-language barrier\n",
    "\n",
    "* **GPU computing Fast-Forward**\n",
    "  * Array vs Kernel programming\n",
    "  * Performance considerations\n",
    "\n",
    "* **Going multi-GPUs**\n",
    "  * MPI + GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why to still bother with GPU computing in 2024\n",
    "- It's around for more than a decade\n",
    "- It shows massive performance gain compared to serial CPU computing\n",
    "- First exascale supercomputer, Frontier, is full of GPUs\n",
    "\n",
    "<img src=\"imgs/frontier.png\" width=400px>\n",
    "\n",
    "### Performance that matters\n",
    "\n",
    "<img src=\"imgs/cpu_gpu_evo.png\" width=600px>\n",
    "\n",
    "Taking a look at a recent GPU and CPU:\n",
    "- Nvidia Tesla A100 GPU\n",
    "- AMD EPYC \"Rome\" 7282 (16 cores) CPU\n",
    "\n",
    "| Device         | TFLOP/s (FP64) | Memory BW TB/s | Imbalance (FP64)     |\n",
    "| :------------: | :------------: | :------------: | :------------------: |\n",
    "| Tesla A100     | 9.7            | 1.55           | 9.7 / 1.55  Ã— 8 = 50 |\n",
    "| AMD EPYC 7282  | 0.7            | 0.085          | 0.7 / 0.085 Ã— 8 = 66 |\n",
    "\n",
    "**Meaning:** we can do about 50 floating point operations per number accessed from main memory.\n",
    "Floating point operations are \"for free\" when we work in memory-bounded regimes.\n",
    "\n",
    "ðŸ‘‰ Requires re-thinking the numerical implementation and solution strategies\n",
    "\n",
    "Unfortunately, the cost of evaluating a first derivative $âˆ‚A / âˆ‚x$ in, e.g., diffusive flux calculations using finite-differences:\n",
    "\n",
    "`q[ix] = -D * (A[ix+1] - A[ix]) / dx`\n",
    "\n",
    "consists of:\n",
    "- 1 read (`A`) + 1 write (`q`) => $2 Ã— 8$ = **16 Bytes transferred**\n",
    "- 1 addition + 1 multiplication + 1 division => **3 floating point operations**\n",
    "\n",
    "ðŸ‘‰ assuming `D`, `dx` are scalars, `q` and `A` are arrays of `Float64` (read from main memory)\n",
    "\n",
    "### Performance that matters - an example\n",
    "Not yet convinced? Let's have a look at an example.\n",
    "\n",
    "Let's assess how close from memory copy (1400 GB/s) we can get solving a 2D diffusion problem on an Nvidia Tesla A100 GPU.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial t} = \\frac{\\partial^2 C}{\\partial x^2} + \\frac{\\partial^2 C}{\\partial y^2} $$\n",
    "\n",
    "ðŸ‘‰ Let's test the performance using a simple script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring GPU performance\n",
    "\n",
    "Load modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using BenchmarkTools\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory copy function to measure the \"peak\" memory throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mycopy! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function mycopy!(A, B)\n",
    "    ix = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "    if ix <= size(A, 1) && iy <= size(A, 2)\n",
    "        @inbounds A[ix, iy] = B[ix, iy] + 1\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian kernel using the finite difference method (FDM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laplacian! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function laplacian!(A, B, dt, _dx2, _dy2)\n",
    "    ix = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "    if ix <= size(A, 1) - 2 && iy <= size(A, 2) - 2\n",
    "        @inbounds A[ix+1, iy+1] = B[ix+1, iy+1] + dt *\n",
    "            ((B[ix+2, iy+1] - 2 * B[ix+1, iy+1] + B[ix, iy+1]) * _dx2 +\n",
    "             (B[ix+1, iy+2] - 2 * B[ix+1, iy+1] + B[ix+1, iy]) * _dy2)\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective memory throughput (copy)      : 1335.85 GB/s\n",
      "Effective memory throughput (laplacian) : 1303.32 GB/s\n",
      "Theoretical peak memory throughput      : 1555.20 GB/s\n",
      "\n",
      "Wow ðŸš€! Laplacian runs at:\n",
      "   97.56% of copy speed\n",
      "   83.80% of peak memory bandwidth\n",
      "on a NVIDIA A100-SXM4-40GB device\n"
     ]
    }
   ],
   "source": [
    "# if the array size is too small, the GPU will not be fully utilized\n",
    "nx = ny = 512 * 32\n",
    "A = CUDA.rand(Float64, nx, ny)\n",
    "B = CUDA.rand(Float64, nx, ny)\n",
    "\n",
    "_dx2 = _dy2 = dt = rand()\n",
    "\n",
    "# launch configuration\n",
    "nthreads = (16, 16)\n",
    "nblocks  = cld.((nx, ny), nthreads)\n",
    "\n",
    "# measure the execution times\n",
    "time_copy = @belapsed CUDA.@sync @cuda threads=nthreads blocks=nblocks mycopy!(A, B)\n",
    "time_lapl = @belapsed CUDA.@sync @cuda threads=nthreads blocks=nblocks laplacian!(A, B, dt, _dx2, _dy2)\n",
    "\n",
    "# effective memory throughput (1 read + 1 write per element)\n",
    "Teff_copy = 2 * nx * ny * sizeof(Float64) / time_copy / 1e9\n",
    "Teff_lapl = 2 * nx * ny * sizeof(Float64) / time_lapl / 1e9\n",
    "\n",
    "# compute theoretical peak memory bandwidth\n",
    "dev = CUDA.device()\n",
    "\n",
    "bus_width       = CUDA.attribute(dev, CUDA.CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH) |> Float64 # in bits\n",
    "clock_rate      = CUDA.attribute(dev, CUDA.CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE)       |> Float64 # in kHz\n",
    "rate_multiplier = 2 # 2 for HBM2/DDR, 4 for HBM3/GDDR5, 8 for GDDR6\n",
    "\n",
    "Teff_peak = bus_width * clock_rate * rate_multiplier / 1e6 / 8\n",
    "\n",
    "# report results\n",
    "@printf(\"Effective memory throughput (copy)      : %.2f GB/s\\n\", Teff_copy)\n",
    "@printf(\"Effective memory throughput (laplacian) : %.2f GB/s\\n\", Teff_lapl)\n",
    "@printf(\"Theoretical peak memory throughput      : %.2f GB/s\\n\", Teff_peak)\n",
    "\n",
    "@printf(\"\\nWow ðŸš€! Laplacian runs at:\\n\")\n",
    "@printf(\"   %.2f%% of copy speed\\n\"           , 100 * Teff_lapl / Teff_copy)\n",
    "@printf(\"   %.2f%% of peak memory bandwidth\\n\", 100 * Teff_lapl / Teff_peak)\n",
    "@printf(\"on a %s device\\n\", CUDA.name(dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU kernel programming\n",
    "\n",
    "We'll get started with a brief overview of the Nvidia GPU architecture and how to program it.\n",
    "\n",
    "The Nvidia general purpose GPUs can be programmed using the CUDA language extension. CUDA is accessible in Julia via [CUDA.jl](https://cuda.juliagpu.org/stable/), which exposes most of the native CUDA features to the Julia ecosystem.\n",
    "\n",
    "In the CUDA programming model, `blocks` of `threads` compose the `grid`. In our implementation, we want to map one thread to each finite-difference cell of the 2D Cartesian domain.\n",
    "\n",
    "The figure hereafter depicts the relation between the CUDA domain and the finite-difference domain:\n",
    "\n",
    "<img src=\"imgs/cuda_grid.png\" width=600px>\n",
    "\n",
    "**Playing with GPUs: the rules**\n",
    "\n",
    "- Current GPUs allow typically a maximum of 1024 threads per block.\n",
    "\n",
    "- The maximum number of blocks allowed is huge; computing the largest possible array on the GPU will make you run out of device memory (currently 16-80 GB) before hitting the maximal number of blocks when selecting sensible kernel launch parameters (usually threads per block >= 128).\n",
    "\n",
    "- Threads, blocks and grid have 3D \"Cartesian\" topology, which is very useful for 1D, 2D and 3D Cartesian finite-difference domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU\n",
    "\n",
    "#### GPU - MPI ranks mapping\n",
    "The challenging part is to run on multiple GPUs using MPI. To achieve this, we need to map node-local MPI ranks to GPU IDs.\n",
    "\n",
    "This can be achieved in Julia using MPI.jl and CUDA.jl by\n",
    "```julia\n",
    "comm   = MPI.COMM_WORLD\n",
    "rank   = MPI.Comm_rank(comm)\n",
    "comm_l = MPI.Comm_split_type(comm, MPI.COMM_TYPE_SHARED, rank)\n",
    "rank_l = MPI.Comm_rank(comm_l)\n",
    "gpu_id = CUDA.device!(rank_l)\n",
    "```\n",
    "\n",
    "#### GPU-aware MPI\n",
    "\n",
    "On modern supercomputers, one has access to GPU-aware MPI. GPU aware-MPI allows to directly exchange GPU memory by-passing an explicit host copy.\n",
    "\n",
    "The file [`multigpu.jl`](./multigpu.jl) implements this and would check that GPU-aware MPI works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "srun: Job 27855189 step creation temporarily disabled, retrying (Requested nodes are busy)\n",
      "srun: Step created for StepId=27855189.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank=3 rank_loc=3 (gpu_id=CuDevice(3)), size=4, dst=0, src=2\n",
      "rank=0 rank_loc=0 (gpu_id=CuDevice(0)), size=4, dst=1, src=3\n",
      "rank=1 rank_loc=1 (gpu_id=CuDevice(1)), size=4, dst=2, src=0\n",
      "rank=2 rank_loc=2 (gpu_id=CuDevice(2)), size=4, dst=3, src=1\n",
      "start sending...\n",
      "recv_mesg on proc 3: [2.0, 2.0, 2.0, 2.0]\n",
      "recv_mesg on proc 0: [3.0, 3.0, 3.0, 3.0]\n",
      "done.\n",
      "recv_mesg on proc 2: [1.0, 1.0, 1.0, 1.0]\n",
      "recv_mesg on proc 1: [0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "run_cmd = `mpiexecjl -n 4 -G 4 --nodes 1 --qos regular --constraint gpu --gpus 4 --account=ntrain1 --project julia multigpu.jl`\n",
    "run(run_cmd);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
