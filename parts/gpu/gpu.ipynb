{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Why to bother with GPU computing in 2024**\n",
    "  * HPC and Supercomputing is GPU-accelerated\n",
    "  * When Julia overcomes the two-language barrier\n",
    "\n",
    "* **GPU computing Fast-Forward**\n",
    "  * Array vs Kernel programming\n",
    "  * Performance considerations\n",
    "\n",
    "* **Going multi-GPUs**\n",
    "  * MPI + GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why to still bother with GPU computing in 2024\n",
    "- It's around for more than a decade\n",
    "- It shows massive performance gain compared to serial CPU computing\n",
    "- First exascale supercomputer, Frontier, is full of GPUs\n",
    "\n",
    "![Frontier](imgs/frontier.png)\n",
    "\n",
    "### Performance that matters\n",
    "\n",
    "![cpu_gpu_evo](imgs/cpu_gpu_evo.png)\n",
    "\n",
    "Taking a look at a recent GPU and CPU:\n",
    "- Nvidia Tesla A100 GPU\n",
    "- AMD EPYC \"Rome\" 7282 (16 cores) CPU\n",
    "\n",
    "| Device         | TFLOP/s (FP64) | Memory BW TB/s | Imbalance (FP64)     |\n",
    "| :------------: | :------------: | :------------: | :------------------: |\n",
    "| Tesla A100     | 9.7            | 1.55           | 9.7 / 1.55  Ã— 8 = 50 |\n",
    "| AMD EPYC 7282  | 0.7            | 0.085          | 0.7 / 0.085 Ã— 8 = 66 |\n",
    "\n",
    "**Meaning:** we can do about 50 floating point operations per number accessed from main memory.\n",
    "Floating point operations are \"for free\" when we work in memory-bounded regimes.\n",
    "\n",
    "ðŸ‘‰ Requires re-thinking the numerical implementation and solution strategies\n",
    "\n",
    "Unfortunately, the cost of evaluating a first derivative $âˆ‚A / âˆ‚x$ in, e.g., diffusive flux calculations using finite-differences:\n",
    "\n",
    "`q[ix] = -D * (A[ix+1] - A[ix]) / dx`\n",
    "\n",
    "consists of:\n",
    "- 1 read (`A`) + 1 write (`q`) => $2 Ã— 8$ = **16 Bytes transferred**\n",
    "- 1 addition + 1 multiplication + 1 division => **3 floating point operations**\n",
    "\n",
    "ðŸ‘‰ assuming `D`, `dx` are scalars, `q` and `A` are arrays of `Float64` (read from main memory)\n",
    "\n",
    "### Performance that matters - an example\n",
    "Not yet convinced? Let's have a look at an example.\n",
    "\n",
    "Let's assess how close from memory copy (1400 GB/s) we can get solving a 2D diffusion problem on an Nvidia Tesla A100 GPU.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial t} = \\frac{\\partial^2 C}{\\partial x^2} + \\frac{\\partial^2 C}{\\partial y^2} $$\n",
    "\n",
    "ðŸ‘‰ Let's test the performance using a simple script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring GPU performance\n",
    "\n",
    "Load modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using BenchmarkTools\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory copy function to measure the \"peak\" memory throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mycopy!(A, B)\n",
    "    ix = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "    if ix <= size(A, 1) && iy <= size(A, 2)\n",
    "        @inbounds A[ix, iy] = B[ix, iy]\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian kernel using the finite difference method (FDM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function laplacian!(A, B, dt, _dx2, _dy2)\n",
    "    ix = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "    if ix <= size(A, 1) - 2 && iy <= size(A, 2) - 2\n",
    "        @inbounds A[ix+1, iy+1] = B[ix+1, iy+1] + dt * \n",
    "            ((B[ix+2, iy+1] - 2 * B[ix+1, iy+1] + B[ix, iy+1]) * _dx2 +\n",
    "             (B[ix+1, iy+2] - 2 * B[ix+1, iy+1] + B[ix+1, iy]) * _dy2)\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the array size is too small, the GPU will not be fully utilized\n",
    "nx = ny = 512 * 32\n",
    "A = CUDA.rand(Float64, nx, ny)\n",
    "B = CUDA.rand(Float64, nx, ny)\n",
    "\n",
    "_dx2 = _dy2 = dt = rand()\n",
    "\n",
    "# launch configuration\n",
    "nthreads = (16, 16)\n",
    "nblocks  = cld.((nx, ny), nthreads)\n",
    "\n",
    "# measure the execution times\n",
    "time_copy = @belapsed CUDA.@sync @cuda threads=nthreads blocks=nblocks mycopy!(A, B)\n",
    "time_lapl = @belapsed CUDA.@sync @cuda threads=nthreads blocks=nblocks laplacian!(A, B, dt, _dx2, _dy2)\n",
    "\n",
    "# effective memory throughput (1 read + 1 write per element)\n",
    "Teff_copy = 2 * nx * ny * sizeof(Float64) / time_copy / 1e9\n",
    "Teff_lapl = 2 * nx * ny * sizeof(Float64) / time_lapl / 1e9\n",
    "\n",
    "# compute theoretical peak memory bandwidth\n",
    "dev  = CUDA.device()\n",
    "\n",
    "bus_width       = CUDA.attribute(dev, CUDA.CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH) |> Float64 # in bits\n",
    "clock_rate      = CUDA.attribute(dev, CUDA.CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE)       |> Float64 # in kHz\n",
    "rate_multiplier = 2 # 2 for HBM2/DDR, 4 for HBM3/GDDR5, 8 for GDDR6\n",
    "\n",
    "Teff_peak = bus_width * clock_rate * rate_multiplier / 1e6 / 8\n",
    "\n",
    "# report results\n",
    "@printf(\"Effective memory throughput (copy)      : %.2f GB/s\\n\", Teff_copy)\n",
    "@printf(\"Effective memory throughput (laplacian) : %.2f GB/s\\n\", Teff_lapl)\n",
    "@printf(\"Theoretical peak memory throughput      : %.2f GB/s\\n\", Teff_peak)\n",
    "\n",
    "@printf(\"\\nWow ðŸš€! Laplacian runs at:\\n\")\n",
    "@printf(\"   %.2f%% of copy speed\\n\"           , 100 * Teff_lapl / Teff_copy)\n",
    "@printf(\"   %.2f%% of peak memory bandwidth\\n\", 100 * Teff_lapl / Teff_peak)\n",
    "@printf(\"on a %s device\\n\", CUDA.name(dev))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
