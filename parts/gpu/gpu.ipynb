{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Why to bother with GPU computing in 2024**\n",
    "  * HPC and Supercomputing is GPU-accelerated\n",
    "  * When Julia overcomes the two-language barrier\n",
    "\n",
    "* **GPU computing Fast-Forward**\n",
    "  * Array vs Kernel programming\n",
    "  * Performance considerations\n",
    "\n",
    "* **Going multi-GPUs**\n",
    "  * MPI + GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why to still bother with GPU computing in 2024\n",
    "- It's around for more than a decade\n",
    "- It shows massive performance gain compared to serial CPU computing\n",
    "- First exascale supercomputer, Frontier, is full of GPUs\n",
    "\n",
    "![Frontier](imgs/frontier.png)\n",
    "\n",
    "### Performance that matters\n",
    "\n",
    "![cpu_gpu_evo](imgs/cpu_gpu_evo.png)\n",
    "\n",
    "Taking a look at a recent GPU and CPU:\n",
    "- Nvidia Tesla A100 GPU\n",
    "- AMD EPYC \"Rome\" 7282 (16 cores) CPU\n",
    "\n",
    "| Device         | TFLOP/s (FP64) | Memory BW TB/s | Imbalance (FP64)     |\n",
    "| :------------: | :------------: | :------------: | :------------------: |\n",
    "| Tesla A100     | 9.7            | 1.55           | 9.7 / 1.55  Ã— 8 = 50 |\n",
    "| AMD EPYC 7282  | 0.7            | 0.085          | 0.7 / 0.085 Ã— 8 = 66 |\n",
    "\n",
    "**Meaning:** we can do about 50 floating point operations per number accessed from main memory.\n",
    "Floating point operations are \"for free\" when we work in memory-bounded regimes.\n",
    "\n",
    "ðŸ‘‰ Requires re-thinking the numerical implementation and solution strategies\n",
    "\n",
    "Unfortunately, the cost of evaluating a first derivative $âˆ‚A / âˆ‚x$ in, e.g., diffusive flux calculations using finite-differences:\n",
    "\n",
    "`q[ix] = -D * (A[ix+1] - A[ix]) / dx`\n",
    "\n",
    "consists of:\n",
    "- 1 read (`A`) + 1 write (`q`) => $2 Ã— 8$ = **16 Bytes transferred**\n",
    "- 1 addition + 1 multiplication + 1 division => **3 floating point operations**\n",
    "\n",
    "ðŸ‘‰ assuming `D`, `dx` are scalars, `q` and `A` are arrays of `Float64` (read from main memory)\n",
    "\n",
    "### Performance that matters - an example\n",
    "Not yet convinced? Let's have a look at an example.\n",
    "\n",
    "Let's assess how close from memory copy (1400 GB/s) we can get solving a 2D diffusion problem on an Nvidia Tesla A100 GPU.\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial t} = \\frac{\\partial^2 C}{\\partial x^2} + \\frac{\\partial^2 C}{\\partial y^2} $$\n",
    "\n",
    "ðŸ‘‰ Let's test the performance using a simple script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring GPU performance\n",
    "\n",
    "Load modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using BenchmarkTools\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory copy function to measure the \"peak\" memory throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mycopy! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function mycopy!(A, B)\n",
    "    ix = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "    if ix <= size(A, 1) && iy <= size(A, 2)\n",
    "        @inbounds A[ix, iy] = B[ix, iy] + 1\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplacian kernel using the finite difference method (FDM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laplacian! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function laplacian!(A, B, dt, _dx2, _dy2)\n",
    "    ix = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    iy = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "    if ix <= size(A, 1) - 2 && iy <= size(A, 2) - 2\n",
    "        @inbounds A[ix+1, iy+1] = B[ix+1, iy+1] + dt *\n",
    "            ((B[ix+2, iy+1] - 2 * B[ix+1, iy+1] + B[ix, iy+1]) * _dx2 +\n",
    "             (B[ix+1, iy+2] - 2 * B[ix+1, iy+1] + B[ix+1, iy]) * _dy2)\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective memory throughput (copy)      : 1335.85 GB/s\n",
      "Effective memory throughput (laplacian) : 1303.32 GB/s\n",
      "Theoretical peak memory throughput      : 1555.20 GB/s\n",
      "\n",
      "Wow ðŸš€! Laplacian runs at:\n",
      "   97.56% of copy speed\n",
      "   83.80% of peak memory bandwidth\n",
      "on a NVIDIA A100-SXM4-40GB device\n"
     ]
    }
   ],
   "source": [
    "# if the array size is too small, the GPU will not be fully utilized\n",
    "nx = ny = 512 * 32\n",
    "A = CUDA.rand(Float64, nx, ny)\n",
    "B = CUDA.rand(Float64, nx, ny)\n",
    "\n",
    "_dx2 = _dy2 = dt = rand()\n",
    "\n",
    "# launch configuration\n",
    "nthreads = (16, 16)\n",
    "nblocks  = cld.((nx, ny), nthreads)\n",
    "\n",
    "# measure the execution times\n",
    "time_copy = @belapsed CUDA.@sync @cuda threads=nthreads blocks=nblocks mycopy!(A, B)\n",
    "time_lapl = @belapsed CUDA.@sync @cuda threads=nthreads blocks=nblocks laplacian!(A, B, dt, _dx2, _dy2)\n",
    "\n",
    "# effective memory throughput (1 read + 1 write per element)\n",
    "Teff_copy = 2 * nx * ny * sizeof(Float64) / time_copy / 1e9\n",
    "Teff_lapl = 2 * nx * ny * sizeof(Float64) / time_lapl / 1e9\n",
    "\n",
    "# compute theoretical peak memory bandwidth\n",
    "dev = CUDA.device()\n",
    "\n",
    "bus_width       = CUDA.attribute(dev, CUDA.CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH) |> Float64 # in bits\n",
    "clock_rate      = CUDA.attribute(dev, CUDA.CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE)       |> Float64 # in kHz\n",
    "rate_multiplier = 2 # 2 for HBM2/DDR, 4 for HBM3/GDDR5, 8 for GDDR6\n",
    "\n",
    "Teff_peak = bus_width * clock_rate * rate_multiplier / 1e6 / 8\n",
    "\n",
    "# report results\n",
    "@printf(\"Effective memory throughput (copy)      : %.2f GB/s\\n\", Teff_copy)\n",
    "@printf(\"Effective memory throughput (laplacian) : %.2f GB/s\\n\", Teff_lapl)\n",
    "@printf(\"Theoretical peak memory throughput      : %.2f GB/s\\n\", Teff_peak)\n",
    "\n",
    "@printf(\"\\nWow ðŸš€! Laplacian runs at:\\n\")\n",
    "@printf(\"   %.2f%% of copy speed\\n\"           , 100 * Teff_lapl / Teff_copy)\n",
    "@printf(\"   %.2f%% of peak memory bandwidth\\n\", 100 * Teff_lapl / Teff_peak)\n",
    "@printf(\"on a %s device\\n\", CUDA.name(dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU\n",
    "\n",
    "#### GPU - MPI ranks mapping\n",
    "The challenging part is to run on multiple GPUs using MPI. To achieve this, we need to map node-local MPI ranks to GPU IDs.\n",
    "\n",
    "This can be achieved in Julia using MPI.jl and CUDA.jl by\n",
    "```julia\n",
    "comm   = MPI.COMM_WORLD\n",
    "rank   = MPI.Comm_rank(comm)\n",
    "comm_l = MPI.Comm_split_type(comm, MPI.COMM_TYPE_SHARED, rank)\n",
    "rank_l = MPI.Comm_rank(comm_l)\n",
    "gpu_id = CUDA.device!(rank_l)\n",
    "```\n",
    "\n",
    "#### GPU-aware MPI\n",
    "\n",
    "On modern supercomputers, one has access to GPU-aware MPI. GPU aware-MPI allows to directly exchange GPU memory by-passing an explicit host copy.\n",
    "\n",
    "The file [`multigpu.jl`](./multigpu.jl) implements this and would check that GPU-aware MPI works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "srun: Job 27855189 step creation temporarily disabled, retrying (Requested nodes are busy)\n",
      "srun: Step created for StepId=27855189.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank=3 rank_loc=3 (gpu_id=CuDevice(3)), size=4, dst=0, src=2\n",
      "rank=0 rank_loc=0 (gpu_id=CuDevice(0)), size=4, dst=1, src=3\n",
      "rank=1 rank_loc=1 (gpu_id=CuDevice(1)), size=4, dst=2, src=0\n",
      "rank=2 rank_loc=2 (gpu_id=CuDevice(2)), size=4, dst=3, src=1\n",
      "start sending...\n",
      "recv_mesg on proc 3: [2.0, 2.0, 2.0, 2.0]\n",
      "recv_mesg on proc 0: [3.0, 3.0, 3.0, 3.0]\n",
      "done.\n",
      "recv_mesg on proc 2: [1.0, 1.0, 1.0, 1.0]\n",
      "recv_mesg on proc 1: [0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# run_cmd = `mpiexecjl -G 4 --project julia multigpu.jl`\n",
    "run_cmd = `mpiexecjl -n 4 -G 4 --nodes 1 --qos regular --constraint gpu --gpus 4 --account=ntrain1 --project julia multigpu.jl`\n",
    "run(run_cmd);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
