{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650f758f-84da-4dd3-9479-8dbc49ebc3d4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "# Setup\n",
    "\n",
    "Note: you might need to run `Pkg.instantiate()` to ensure that the `Manifest.toml` is up to date. This only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ab4e89-10ca-4ba8-a7bc-d33fcf3f2e60",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `/global/u1/b/blaschke/juliacon24-hpcworkshop/parts/mpi/explanation`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `/global/u1/b/blaschke/juliacon24-hpcworkshop/parts/mpi/explanation/Project.toml`\n",
      "  \u001b[90m[1520ce14] \u001b[39mAbstractTrees v0.4.5\n",
      "  \u001b[90m[052768ef] \u001b[39mCUDA v5.4.2\n",
      "  \u001b[90m[adafc99b] \u001b[39mCpuId v0.3.1\n",
      "  \u001b[90m[0e44f5e4] \u001b[39mHwloc v3.0.1\n",
      "  \u001b[90m[da04e1cc] \u001b[39mMPI v0.20.20\n",
      "  \u001b[90m[e7922434] \u001b[39mMPIClusterManagers v0.2.4\n",
      "  \u001b[90m[6f74fd91] \u001b[39mNetworkInterfaceControllers v0.1.0\n"
     ]
    }
   ],
   "source": [
    "import Pkg;\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53799c57-9c82-4cb2-9a73-f858a8725071",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Julia + Jupyter + MPI\n",
    "\n",
    "`MPI.jl` provides wrappers for the system MPI libraries. And the `MPIClusterManagers.jl` package lets you control MPI workflows within Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cfa159-4234-4961-b18e-6f7a4472bb04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## MPI.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bcb1ba8-c4da-4311-a873-3354126c952d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4228e3-d910-451b-8523-7b60f342788d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`MPI.versioninfo()` tells you which MPI backend is being used by `MPI.jl`. On HPC systems, which rely on vendor-provided MPI implementations (e.g. on HPE Cray systems like Perlmutter), make sure that `MPI.jl` loads the \"right\" `libmpi.so`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4f99e3-63a2-43af-903d-36cfbe011415",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPIPreferences:\n",
      "  binary:  system\n",
      "  abi:     MPICH\n",
      "  libmpi:  libmpi_gnu_123.so\n",
      "  mpiexec: srun\n",
      "\n",
      "Package versions\n",
      "  MPI.jl:             0.20.20\n",
      "  MPIPreferences.jl:  0.1.11\n",
      "\n",
      "Library information:\n",
      "  libmpi:  libmpi_gnu_123.so\n",
      "  libmpi dlpath:  /opt/cray/pe/lib64/libmpi_gnu_123.so\n",
      "  MPI version:  3.1.0\n",
      "  Library version:  \n",
      "    MPI VERSION    : CRAY MPICH version 8.1.28.29 (ANL base 3.4a2)\n",
      "    MPI BUILD INFO : Wed Nov 15 20:57 2023 (git hash 1cde46f)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "MPI.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcbfaa-839b-4d40-a9ef-fc99cee61b04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## MPIClusterManagers.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338abb9b-48de-4c85-9e82-bc08927ad43a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "`MPIClusterManagers.jl` provide a way for Jupyter to connect to MPI processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725708a-b5b5-4cac-8983-c95a0c4b7ab9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "On Perlmutter, we have a choice among network interfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e41152-6380-4b21-8bbe-71257eb8aba7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Vector{NetworkInterfaceControllers.Interface}:\n",
       " NetworkInterfaceControllers.Interface(\"nmn0\", :v4, ip\"10.100.108.57\")\n",
       " NetworkInterfaceControllers.Interface(\"hsn0\", :v4, ip\"10.249.42.35\")\n",
       " NetworkInterfaceControllers.Interface(\"hsn0:chn\", :v4, ip\"128.55.84.171\")\n",
       " NetworkInterfaceControllers.Interface(\"hsn1\", :v4, ip\"10.249.42.19\")\n",
       " NetworkInterfaceControllers.Interface(\"hsn2\", :v4, ip\"10.249.42.20\")\n",
       " NetworkInterfaceControllers.Interface(\"hsn3\", :v4, ip\"10.249.42.36\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using NetworkInterfaceControllers, Sockets\n",
    "interfaces = NetworkInterfaceControllers.get_interface_data(IPv4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c91aa1-41ce-450a-b646-d8574e8740f4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Buf we have to be careful about which network we connect to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31df2d1-6a35-4420-9385-b60af0831074",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filter (generic function with 11 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: filter, Fix1\n",
    "filter(f::Function)::Function = Fix1(filter, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e0a840-7b61-4202-974c-1cda95820690",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Hwloc, AbstractTrees\n",
    "\n",
    "import AbstractTrees: PreOrderDFS\n",
    "import Hwloc: hwloc_pci_class_string\n",
    "\n",
    "sys_devs = children(gettopology())\n",
    "pci_devs = PreOrderDFS(sys_devs) |> collect |> filter(x->x.type==:PCI_Device)\n",
    "net_devs = pci_devs |> filter(x->hwloc_pci_class_string(nodevalue(x).attr.class_id) == \"Ethernet\")\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848daddc-d8cb-4ad0-9a33-eed34197e3cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device hsn0 is a Slingshot device\n",
      "Device nmn0 is a Unknown device\n",
      "Device hsn1 is a Slingshot device\n",
      "Device hsn2 is a Slingshot device\n",
      "Device hsn3 is a Slingshot device\n"
     ]
    }
   ],
   "source": [
    "# net_devs are populated using Hwloc, please take a look at the source notebook\n",
    "# for further information\n",
    "\n",
    "for dev in net_devs\n",
    "    io = dev.io_children |> only\n",
    "    name = io.object.name\n",
    "    kind = io.object.subtype\n",
    "    kind = kind == \"\" ? \"Unknown\" : kind\n",
    "    println(\"Device $(name) is a $(kind) device\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb812b-3779-48ae-a982-d3aa8599b39f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Therefore only the `hsn*` defivices are Slingshot devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d965b3-1002-41ec-a964-6e4f71faf95e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's now use this information to find a HSN device with which we manage our MPI cluster. Note: we'll take the one with `:chn` in the name (as it's the only one with a public IP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6bdb63-1f0e-4bf6-ad6a-144d365a7e97",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkInterfaceControllers.Interface(\"hsn0:chn\", :v4, ip\"128.55.84.171\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsn0_public = filter(\n",
    "    x->(x.name==\"hsn0:chn\" && x.version==:v4), interfaces\n",
    ") |> only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a502b97-b4e1-44f9-a5e9-3bc09c0e8491",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"nid200344-hsn0\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_slingshot_name = getnameinfo(hsn0_public.ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db6ae1-a001-4606-9933-55f2ac158be2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## MPI Worker Cluster\n",
    "\n",
    "We use `MPIClusterManagers.jl` to start a cluster of workers. Each worker uses MPI to communicate (`MPIWorkerManager` stars an `srun` session), and is controlled via the device at `public_slingshot_name` (previous section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c81c337-5e88-4688-bcf2-f48b6eeb98e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to import MPIManager\n",
    "using MPIClusterManagers\n",
    "\n",
    "# need to also import Distributed to use addprocs()\n",
    "using Distributed\n",
    "\n",
    "# specify, number of mpi workers, launch cmd, etc.\n",
    "manager=MPIWorkerManager(4)\n",
    "\n",
    "# start mpi workers and add them as julia workers too.\n",
    "addprocs(\n",
    "    manager,\n",
    "    exeflags=`--project=$(Base.active_project())`,\n",
    "    master_tcp_interface=public_slingshot_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ca90a-f66e-43d6-a887-2b6956fae59e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now we can use `@mpi_do` to issue instructions to all of our MPI workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f6bc5b9-2973-4dc5-8fdd-bfd483f01460",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 5:\tHello world, I am 3 of 4 on nid200349\n",
      "      From worker 4:\tHello world, I am 2 of 4 on nid200348\n",
      "      From worker 2:\tHello world, I am 0 of 4 on nid200344\n",
      "      From worker 3:\tHello world, I am 1 of 4 on nid200345\n"
     ]
    }
   ],
   "source": [
    "@mpi_do manager begin\n",
    "    using MPI: MPI, Comm, Win, free\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = MPI.Comm_rank(comm)\n",
    "    size = MPI.Comm_size(comm)\n",
    "    name = gethostname()\n",
    "    println(\"Hello world, I am $(rank) of $(size) on $(name)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98174d30-5828-43f9-b63d-11d85a46185c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We started this in a 4-node job. Therefore each worker is on a different node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e46e3b-f8d4-48d5-b8fc-2ae660f5a4a8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
