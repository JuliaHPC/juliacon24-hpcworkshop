{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71670946-bcb3-4772-9656-7dddb8a202b7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ab4e89-10ca-4ba8-a7bc-d33fcf3f2e60",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `/global/u1/b/blaschke/juliacon24-hpcworkshop/parts/mpi/explanation`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `/global/u1/b/blaschke/juliacon24-hpcworkshop/parts/mpi/explanation/Project.toml`\n",
      "  \u001b[90m[1520ce14] \u001b[39mAbstractTrees v0.4.5\n",
      "  \u001b[90m[0e44f5e4] \u001b[39mHwloc v3.0.1\n",
      "  \u001b[90m[da04e1cc] \u001b[39mMPI v0.20.20\n",
      "  \u001b[90m[e7922434] \u001b[39mMPIClusterManagers v0.2.4\n",
      "  \u001b[90m[6f74fd91] \u001b[39mNetworkInterfaceControllers v0.1.0\n"
     ]
    }
   ],
   "source": [
    "import Pkg;\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c81c337-5e88-4688-bcf2-f48b6eeb98e8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyError",
     "evalue": "KeyError: key \"usage_request\" not found",
     "output_type": "error",
     "traceback": [
      "KERNEL EXCEPTION",
      "KeyError: key \"usage_request\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{String, Function}, key::String)",
      "   @ Base ./dict.jl:484",
      " [2] eventloop(socket::ZMQ.Socket)",
      "   @ IJulia ~/.julia/packages/IJulia/Vo51o/src/eventloop.jl:8",
      " [3] (::IJulia.var\"#14#17\")()",
      "   @ IJulia ./task.jl:514"
     ]
    }
   ],
   "source": [
    "using MPI\n",
    "\n",
    "using NetworkInterfaceControllers, Sockets\n",
    "interfaces = NetworkInterfaceControllers.get_interface_data(IPv4)\n",
    "\n",
    "hsn0_public = filter(x->(x.name==\"hsn0:chn\" && x.version==:v4), interfaces) |> only \n",
    "public_slingshot_name = getnameinfo(hsn0_public.ip)\n",
    "\n",
    "# to import MPIManager\n",
    "using MPIClusterManagers\n",
    "\n",
    "# need to also import Distributed to use addprocs()\n",
    "using Distributed\n",
    "\n",
    "# specify, number of mpi workers, launch cmd, etc.\n",
    "manager=MPIWorkerManager(4)\n",
    "\n",
    "# start mpi workers and add them as julia workers too.\n",
    "addprocs(\n",
    "    manager,\n",
    "    exeflags=`--project=$(Base.active_project())`,\n",
    "    master_tcp_interface=public_slingshot_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53799c57-9c82-4cb2-9a73-f858a8725071",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Communication with MPI.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332001ad-3b08-4ceb-b4e4-54e619451191",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Picking up from the previous demo, we have a job with 4 ranks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6bc5b9-2973-4dc5-8fdd-bfd483f01460",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 5:\tHello world, I am 3 of 4 on nid200453\n",
      "      From worker 2:\tHello world, I am 0 of 4 on nid200448\n",
      "      From worker 3:\tHello world, I am 1 of 4 on nid200449\n",
      "      From worker 4:\tHello world, I am 2 of 4 on nid200452\n"
     ]
    }
   ],
   "source": [
    "@mpi_do manager begin\n",
    "    using MPI: MPI, Comm, Win, free\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = MPI.Comm_rank(comm)\n",
    "    size = MPI.Comm_size(comm)\n",
    "    name = gethostname()\n",
    "    println(\"Hello world, I am $(rank) of $(size) on $(name)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982d349-c25e-4bc9-9624-bbf6f2b6c8cc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Domain Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5872e-ab53-4871-8bf0-be59956fd42e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "PDE solvers often break up work over a \"grid\" of ranks (domain decomposition). This will find the dimension of this grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1122c61b-aa2b-47e5-871f-ea7f2f1d501b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    dims = [0]\n",
    "    MPI.Dims_create!(size, dims)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612ccdb8-8e29-41cc-8c1f-af533e355715",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 2:\t[4]\n",
      "      From worker 5:\t[4]\n",
      "      From worker 3:\t[4]\n",
      "      From worker 4:\t[4]\n"
     ]
    }
   ],
   "source": [
    "@mpi_do manager begin\n",
    "    println(dims)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec74bff-4668-4c33-b93a-ee19f67551ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Each rank has the same value for `dims`. In $N$-dimensions, `length(dims) == N`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3679ec-dfac-46d7-97ef-e0ad10ffe295",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Cartesian Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f8fd5-7504-4b03-9a62-e63d3278d098",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We will now lay out each rank in a \"grid\" (in this example, $N=1$ so it's actually a line. In the excercise, $N=2$, so this will be an actual \"grid\". The steps here are pretty much the same though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33bfb02-e341-40e4-8315-83734796a18b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    comm_cart = MPI.Cart_create(\n",
    "        comm,  # MPI Communicator\n",
    "        dims,  # Dimensions of grid\n",
    "        [0],   # 0 == not periodic, 1 == periodic\n",
    "        1,     # 0 == not allowed to reorder, 1 == allowed to reoder\n",
    "    )\n",
    "    me        = MPI.Comm_rank(comm_cart)\n",
    "    coords    = MPI.Cart_coords(comm_cart)\n",
    "    neighbors = MPI.Cart_shift(\n",
    "        comm_cart,\n",
    "        0,  # Which dimension to shift (zero-indexed)\n",
    "        1,  # Shift magnitude\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8cf1293-b416-415f-a14e-d529a9e3e7bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    comm_cart = MPI.Cart_create(\n",
    "        comm,  # MPI Communicator\n",
    "        dims,  # Dimensions of grid\n",
    "        [0],   # 0 == not periodic, 1 == periodic\n",
    "        1,     # 0 == not allowed to reorder, 1 == allowed to reoder\n",
    "    )\n",
    "    me        = MPI.Comm_rank(comm_cart)\n",
    "    coords    = MPI.Cart_coords(comm_cart)\n",
    "    neighbors = MPI.Cart_shift(\n",
    "        comm_cart,\n",
    "        0,  # Which dimension to shift (zero-indexed)\n",
    "        1,  # Shift magnitude\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3ab1a58-0aea-4ec5-a79b-48bcd810c631",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\trank=1; coord=[1], neighbors=(0, 2)\n",
      "      From worker 2:\trank=0; coord=[0], neighbors=(-1, 1)\n",
      "      From worker 4:\trank=2; coord=[2], neighbors=(1, 3)\n",
      "      From worker 5:\trank=3; coord=[3], neighbors=(2, -1)\n"
     ]
    }
   ],
   "source": [
    "@mpi_do manager begin\n",
    "    println(\"rank=$(me); coord=$(coords), neighbors=$(neighbors)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63bda425-3a47-4a1c-ba8b-ae3c891d3021",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 2:\trank=0; coord=[0], neighbors=(-1, 1)\n",
      "      From worker 3:\trank=1; coord=[1], neighbors=(0, 2)\n",
      "      From worker 5:\trank=3; coord=[3], neighbors=(2, -1)\n",
      "      From worker 4:\trank=2; coord=[2], neighbors=(1, 3)\n"
     ]
    }
   ],
   "source": [
    "@mpi_do manager begin\n",
    "    println(\"rank=$(me); coord=$(coords), neighbors=$(neighbors)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b410a-c68c-4e38-ab1c-e355c4d20d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "MPI contains several constants, for example what `-1` means in the context above. This means that there is \"no neighbor\" there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94bc63d1-24cc-47f6-a6ab-4624d95523fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MPI.PROC_NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165e80a-91ce-4233-a8e4-4bd3f09786c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Point-to-point Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f6f278-6dc3-4042-aa06-4abc9a7fa7f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's do something harder:\n",
    "1. Each rank draws a random number between 1 and 100\n",
    "2. Each rank's random number is shared with its neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286f218-4851-4f11-b3e2-550635a2c688",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This is an example of point-to-point communication on a grid. We'll be using the same communication pattern in the excercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45478166-3101-4380-9149-e9ee101b3b06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "First we generate a andom number on each rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5187bd3-8699-4a3b-a43c-28d4a647cdc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    using Random\n",
    "    my_int = rand(1:100)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a926edfd-9b22-4e33-851d-6d9e26429065",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\trank=1; my_int=21\n",
      "      From worker 4:\trank=2; my_int=37\n",
      "      From worker 5:\trank=3; my_int=83\n",
      "      From worker 2:\trank=0; my_int=100\n"
     ]
    }
   ],
   "source": [
    "@mpi_do manager begin\n",
    "    println(\"rank=$(me); my_int=$(my_int)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d74de-4b8a-4962-a521-f620f8164cae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "MPI uses zero-copy memory access => we need to set up buffers (arrays) to send and receive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "343bd286-e07b-49b6-8342-ebd85b1a2af7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    send_1 = zeros(Int64, 1)\n",
    "    send_2 = zeros(Int64, 1)\n",
    "    recv_1 = zeros(Int64, 1)\n",
    "    recv_2 = zeros(Int64, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669bf32-cc11-42b3-b353-31b3231999b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Now we fill the buffers by copying out data into it -- wherever a buffer is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48a0fa62-2cd2-4071-9046-958e0b335916",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    if neighbors[1] != MPI.PROC_NULL\n",
    "        copyto!(send_1, my_int)\n",
    "    end\n",
    "    if neighbors[2] != MPI.PROC_NULL\n",
    "        copyto!(send_2, my_int)\n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79dfa66-e9c8-455f-b658-004e49ea4df2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now we're ready to perform a data transfer with MPI. MPI is (largely) transaction based. There is a receiving end, and a sending end. In order for a send to be successful, the receiver must be ready to receive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89f9e2-2527-4700-9eeb-600c1844eb06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "To help coordinate all of this, we set up a request store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c05abe1a-8d67-4aff-9191-d135272ca4be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    reqs = MPI.MultiRequest(4)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256d83d-f6fe-4bed-88d0-e405f53dd664",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "And we transfer the data using non-blocking MPI communivation (`Isend` and `Irecv`). Pro tip: initiate receive before send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d847d757-71b4-4d62-8faa-228962bb4794",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    # \n",
    "    if neighbors[1] != MPI.PROC_NULL\n",
    "        MPI.Irecv!(recv_1, comm, reqs[1]; source=neighbors[1])\n",
    "    end\n",
    "    if neighbors[2] != MPI.PROC_NULL\n",
    "        MPI.Irecv!(recv_2, comm, reqs[2]; source=neighbors[2])\n",
    "    end\n",
    "    # Send data\n",
    "    if neighbors[1] != MPI.PROC_NULL\n",
    "        MPI.Isend(send_1, comm, reqs[3]; dest=neighbors[1])\n",
    "    end\n",
    "    if neighbors[2] != MPI.PROC_NULL\n",
    "        MPI.Isend(send_2, comm, reqs[4]; dest=neighbors[2])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ef6ff-dead-4aff-981c-21407f01c9ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Notice how we tagged data with `source` and `dest`. This makes sure that data is received in the correct order (the middle ranks receive data from _both_ sides), and -- in the case of `Isend` -- that the data is sent to the correct rank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed66db-1274-4efe-a2a0-a8b2b7986527",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "When using non-blocking communication, it's good to wait for all transactions to be completed before using the buffers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "113d8a31-1834-4d6d-931f-3991592e7ab5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@mpi_do manager begin\n",
    "    # Wait for all requests to finish\n",
    "    MPI.Waitall(reqs)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e5ed91-9afd-4764-b875-ebbf924dc077",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's take a look at what we've transferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7f159d3-e651-4795-b63b-2b49a03af961",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 2:\trank=0; my_int=100; prev=[0]; next=[21]\n",
      "      From worker 5:\trank=3; my_int=83; prev=[37]; next=[0]\n",
      "      From worker 4:\trank=2; my_int=37; prev=[21]; next=[83]\n",
      "      From worker 3:\trank=1; my_int=21; prev=[100]; next=[37]\n"
     ]
    }
   ],
   "source": [
    "@mpi_do manager begin\n",
    "    println(\"rank=$(me); my_int=$(my_int); prev=$(recv_1); next=$(recv_2)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e46e3b-f8d4-48d5-b8fc-2ae660f5a4a8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
