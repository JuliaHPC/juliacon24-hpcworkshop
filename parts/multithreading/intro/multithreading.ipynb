{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What are threads?**\n",
    "\n",
    "* **How many threads are there?**\n",
    "\n",
    "* **Where are the threads running?**\n",
    "\n",
    "* **How do I use the threads?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are threads?\n",
    "Threads are **execution units within a process** that can run simultaneously and **share memory** (heap).\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/stack_heap_threads.png\" width=450px>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many threads are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia starts with a single *user thread*. We must tell it explicitly to start multiple user threads.\n",
    "\n",
    "* Environment variable: `export JULIA_NUM_THREADS=8`\n",
    "\n",
    "* Command line argument: `julia -t 8` or `julia --threads 8`\n",
    "\n",
    "* **VS Code:** Add `\"julia.NumThreads\": 8` to workspace settings (`Preferences: Open Workspace Settings (JSON)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is currently not really possible to change the number of threads at runtime!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are the threads running?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ThreadPinning.jl](https://github.com/carstenbauer/ThreadPinning.jl) is the best tool for visualizing and controlling thread placement in Julia. (Disclaimer: I'm the main author üòâ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ThreadPinning\n",
    "\n",
    "threadinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinning threads (i.e. controling where they are running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why?\n",
    "\n",
    "* To avoid double occupancy of CPU cores.\n",
    "\n",
    "* To reduce noise in benchmarks.\n",
    "\n",
    "* To address the complexity of the system topology, e.g. to use specific/all memory domains (NUMA).\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How?\n",
    "\n",
    "`pinthreads(strategy)`\n",
    "* `:cputhreads` pin to CPU threads (incl. \"hypterthreads\") one after another\n",
    "* `:cores:` pin to CPU cores one after another\n",
    "* `:numa:` alternate between NUMA domains (round-robin)\n",
    "* `:sockets:` alternate between sockets (round-robin)\n",
    "* `:affinitymask`: pin according to an external affinity mask (e.g. set by SLURM)\n",
    "\n",
    "(More? See my talk at JuliaCon2023 @ MIT: https://youtu.be/6Whc9XtlCC0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinthreads(:cores) # try :cores or :sockets or :random\n",
    "threadinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinthreads(:numa)\n",
    "threadinfo(; groupby=:numa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory domains (NUMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUMA = **n**on-**u**niform **m**emory **a**ccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One (of two) AMD Milan CPUs in a Perlmutter node:\n",
    "\n",
    "<img src=\"imgs/amd_milan_cpu_die.svg\" width=800px>\n",
    "\n",
    "**Image source:** [AMD, High Performance Computing (HPC) Tuning Guide for AMD EPYCTM 7003 Series Processors](https://www.amd.com/system/files/documents/high-performance-computing-tuning-guide-amd-epyc7003-series-processors.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other useful options for querying system information\n",
    "\n",
    "# using CpuId\n",
    "# cpuinfo()\n",
    "\n",
    "# using Hwloc\n",
    "# topology_graphical()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I use the threads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-based multithreading\n",
    "\n",
    "In traditional HPC, one typically tells each thread what to do. **(\"One thinks about threads\")**\n",
    "\n",
    "Julia implements **task-based multithreading**. In this paradigm, a task - e.g. a computational piece of a code - is marked for **parallel** execution on **any** of the available Julia threads. Julia's **dynamic scheduler** will automatically put the task on one of the threads and trigger the execution of the task on said thread.\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/tasks_threads_cores.svg\" width=750px>\n",
    "</br>\n",
    "\n",
    "Generally speaking, the user should **think about tasks and not threads**.\n",
    "* The scheduler is controlling on which thread a task will eventually run.\n",
    "* It might even dynamically [migrate tasks](https://docs.julialang.org/en/v1/manual/multi-threading/#man-task-migration) between threads.\n",
    "\n",
    "**Advantages:**\n",
    "* high-level abstraction\n",
    "* nestability / composability (especially important for libraries)\n",
    "\n",
    "**Disadvantages:**\n",
    "* scheduling overhead\n",
    "* uncertain and potentially suboptimal task ‚Üí thread assignment\n",
    "  * **can get in the way when performance engineering** because\n",
    "    * scheduler has limited information (e.g. about the system topology)\n",
    "    * profiling tools often don't know anything about tasks but monitor threads (or even CPU-cores) instead (e.g. LIKWID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `@threads for ... in ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Splits up the iteration space into `nthreads()` contiguous chunks**\n",
    "\n",
    "* Creates a task for each of them and hands them off to the dynamic scheduler (essentially `@spawn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Threads: @threads, threadid, nthreads\n",
    "using ThreadPinning: taskid # for pedagogical purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates nthreads() many tasks\n",
    "\n",
    "@threads for i in 1:2*nthreads()\n",
    "    println(\"Task \", ThreadPinning.taskid(), \" is running iteration \", i, \" on thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static scheduling: `@threads :static for ... in ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `:static` scheduling option to opt-out of Julia's dynamic scheduling\n",
    "\n",
    "* Statically **\"pins\" tasks to threads**\n",
    "  * task 1 ‚Üí thread 1, task 2 ‚Üí thread 2, and so on.\n",
    "\n",
    "Pro\n",
    "   * no task migration, i.e. **fixed task-thread mapping** üëç\n",
    "   \n",
    "Con\n",
    "   * only little overhead üëç\n",
    "   * not composable / nestable üëé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads :static for i in 1:2*nthreads()\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid());\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `@threads :static`, every thread handles precisely two iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Data pinning\" (NUMA revisited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicitly ‚Üí **NUMA \"first-touch\" policy**\n",
    "\n",
    "Explicitly ‚Üí [NUMA.jl](https://github.com/JuliaPerf/NUMA.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMA \"first-touch\" policy\n",
    "\n",
    "Data is (typically) placed in the **NUMA domain that is closest to the thread/CPU core** that is \"touching\" the data.\n",
    "\n",
    "```julia\n",
    "x = Vector{Float64}(undef, 10)   # allocation, no \"touch\" yet\n",
    "rand!(x)                         # first touch == first write\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinthreads(:numa)\n",
    "threadinfo(; groupby=:numa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array initialization: serial vs parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different parts of an array can be placed in different NUMA domains!**\n",
    "\n",
    "(Data is managed in terms of smaller units, namley memory pages.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serial\n",
    "\n",
    "```julia\n",
    "x = Vector{Float64}(undef, 100)   # allocation, no \"touch\" yet\n",
    "rand!(x)                          # first touch == first write\n",
    "```\n",
    "\n",
    "The location of the \"main\" thread determines the NUMA domain of the entire array!\n",
    "\n",
    "If we later access the data in parallel, all threads must read from the same NUMA domain ‚Üí competition for the memory bus ‚Üí potential bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel\n",
    "\n",
    "```julia\n",
    "pinthreads(:numa)                       # pin threads to different NUMA domains\n",
    "x = Vector{Float64}(undef, 100)         # allocation, no \"touch\" yet\n",
    "@threads :static for i in eachindex(x)  # parallel iteration\n",
    "    x[i] = rand()                       # first touch == first write\n",
    "end\n",
    "```\n",
    "\n",
    "Different threads - running in different NUMA regions - touch different parts of the array ‚Üí the latter will (likely) be placed in different NUMA domains.\n",
    "\n",
    "If we later access the data in parallel, all threads can read their part of the array from their local NUMA domain ‚Üí no bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crucial point: How you initialize your data influences the performance of your computational kernel(s)!**\n",
    "* up to a factor of 8 performance difference possible on Perlmutter\n",
    "\n",
    "* non-local performance effect ‚Üí hard to debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
